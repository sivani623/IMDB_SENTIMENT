# -*- coding: utf-8 -*-
"""IMDB_Sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11db0qNnxuPELlsoqdjney3J8NMNsuyH_

**Setup & Utils**
"""

import os, re, time, math, random, psutil, platform, gc, string
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# We'll use Keras Tokenizer only for text -> int ids & trunc/vocab=10k (as allowed in brief).
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# ==== Reproducibility ====
torch.manual_seed(42); np.random.seed(42); random.seed(42)  # per brief
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ==== Hardware report (per brief) ====
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
def hardware_report():
    print("Device:", device)
    print("CPU:", platform.processor() or platform.platform())
    print("RAM (GB):", round(psutil.virtual_memory().total/1e9, 2))
    if device.type == "cuda":
        print("CUDA:", torch.version.cuda, "| GPU:", torch.cuda.get_device_name(0))
hardware_report()

"""**Dataset Preparation**"""

# ==== Paths ====
DATA_CSV = "/content/IMDB_Dataset.csv"  # Make sure your uploaded filename matches exactly
assert os.path.exists(DATA_CSV), f"CSV not found at {DATA_CSV}. Upload it to Colab Files."

# ==== Load ====
df = pd.read_csv(DATA_CSV)

df = df.rename(columns={c:c.lower() for c in df.columns})
assert 'review' in df.columns and 'sentiment' in df.columns, "CSV must have 'review' and 'sentiment' columns."

# ==== Clean text (lowercase, remove punctuation/special chars) ====
def clean_text(s):
    s = str(s).lower()
    s = re.sub(r"<br\s*/?>", " ", s)           # remove html breaks
    s = re.sub(r"http\S+|www\S+", " ", s)      # remove urls
    s = re.sub(r"[^a-z\s']", " ", s)           # keep letters/apostrophes/spaces
    s = re.sub(r"\s+", " ", s).strip()
    return s

df['text'] = df['review'].map(clean_text)
df['label'] = (df['sentiment'].str.lower() == 'positive').astype(int)

# ==== 50/50 split (25k/25k) ====
# Kaggle CSV is not pre-split; we create a stratified 50/50 split to match the requirement.
train_text, test_text, train_y, test_y = train_test_split(
    df['text'].values, df['label'].values, test_size=0.5, random_state=42, stratify=df['label'].values
)

# ==== Tokenize (top 10,000 words) ====
VOCAB_SIZE = 10_000
tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token="<OOV>")
tokenizer.fit_on_texts(train_text)  # fit on train only

# Utility to make (X, y) numpy arrays at a given sequence length
def make_xy(texts, labels, maxlen):
    seqs = tokenizer.texts_to_sequences(texts)
    X = pad_sequences(seqs, maxlen=maxlen, truncating='post', padding='post')  # pad/truncate to fixed lengths
    y = np.asarray(labels, dtype=np.int64)
    return X, y

print("Data ready. Example cleaned text:\n", train_text[0][:200], "...\n")
print("Vocab size (limited):", VOCAB_SIZE)

class SeqDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.float32)  # BCE expects float target
    def __len__(self): return len(self.y)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

def make_loaders(seq_len, batch_size=32):
    Xtr, ytr = make_xy(train_text, train_y, seq_len)
    Xte, yte = make_xy(test_text,  test_y,  seq_len)
    train_ds = SeqDataset(Xtr, ytr); test_ds = SeqDataset(Xte, yte)
    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)
    test_ld  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False)
    return train_ld, test_ld

"""**Model Architecture**

"""

EMB_DIM   = 100
HID_DIM   = 64
NUM_LAYERS= 2
DROPOUT   = 0.4    # within 0.3â€“0.5
PAD_IDX   = 0

# activation "head" after encoder output (so we can vary sigmoid/relu/tanh as required)
ACT_LOOKUP = {
    "relu": nn.ReLU(),
    "tanh": nn.Tanh(),
    "sigmoid": nn.Sigmoid(),
}

class BaseEncoder(nn.Module):
    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_IDX)
        self.dropout = nn.Dropout(dropout)

class RNNEncoder(BaseEncoder):
    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout, bidirectional=False):
        super().__init__(vocab_size, emb_dim, hid_dim, num_layers, dropout)
        self.rnn = nn.RNN(emb_dim, hid_dim, num_layers=num_layers, batch_first=True,
                          nonlinearity='tanh', dropout=dropout, bidirectional=bidirectional)
        self.bi = 2 if bidirectional else 1
    def forward(self, x):
        x = self.dropout(self.emb(x))
        out, h_n = self.rnn(x)                # h_n: [num_layers * bi, B, H]
        h_last = h_n[-self.bi:].transpose(0,1).reshape(x.size(0), -1)  # concat last layer's dirs
        return h_last

class LSTMEncoder(BaseEncoder):
    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout, bidirectional=False):
        super().__init__(vocab_size, emb_dim, hid_dim, num_layers, dropout)
        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True,
                            dropout=dropout, bidirectional=bidirectional)
        self.bi = 2 if bidirectional else 1
    def forward(self, x):
        x = self.dropout(self.emb(x))
        out, (h_n, c_n) = self.lstm(x)
        h_last = h_n[-self.bi:].transpose(0,1).reshape(x.size(0), -1)
        return h_last

class SentimentModel(nn.Module):
    def __init__(self, arch="lstm", activation="relu", bidirectional=False):
        super().__init__()
        arch = arch.lower(); activation = activation.lower()
        if arch == "rnn":
            self.encoder = RNNEncoder(VOCAB_SIZE, EMB_DIM, HID_DIM, NUM_LAYERS, DROPOUT, bidirectional=bidirectional)
        elif arch == "lstm":
            self.encoder = LSTMEncoder(VOCAB_SIZE, EMB_DIM, HID_DIM, NUM_LAYERS, DROPOUT, bidirectional=bidirectional)
        elif arch == "bilstm":
            self.encoder = LSTMEncoder(VOCAB_SIZE, EMB_DIM, HID_DIM, NUM_LAYERS, DROPOUT, bidirectional=True)
        else:
            raise ValueError("arch must be 'rnn', 'lstm', or 'bilstm'")

        bi = 2 if (arch == "bilstm" or getattr(self.encoder, "bi", 1) == 2) else 1
        hidden_out = HID_DIM * bi
        self.head = nn.Sequential(
            nn.Dropout(DROPOUT),
            nn.Linear(hidden_out, hidden_out),
            ACT_LOOKUP.get(activation, nn.ReLU()),
            nn.Dropout(DROPOUT),
            nn.Linear(hidden_out, 1),
            nn.Sigmoid()   # sigmoid output for binary classification
        )

    def forward(self, x):
        feat = self.encoder(x)
        return self.head(feat).squeeze(1)

"""**Evaluation Experiments**

"""

def train_one_epoch(model, loader, optimizer, criterion, clip_norm=None):
    model.train()
    t0 = time.time()
    losses = []
    for xb, yb in loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad(set_to_none=True)
        preds = model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        if clip_norm is not None:
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)
        optimizer.step()
        losses.append(loss.item())
    return np.mean(losses), time.time() - t0

@torch.no_grad()
def evaluate(model, loader, criterion):
    model.eval()
    losses, y_true, y_prob = [], [], []
    for xb, yb in loader:
        xb, yb = xb.to(device), yb.to(device)
        preds = model(xb)
        loss = criterion(preds, yb)
        losses.append(loss.item())
        y_true.extend(yb.cpu().numpy().tolist())
        y_prob.extend(preds.cpu().numpy().tolist())
    y_pred = (np.array(y_prob) >= 0.5).astype(int)
    acc = accuracy_score(y_true, y_pred)
    f1  = f1_score(y_true, y_pred, average="macro")
    return np.mean(losses), acc, f1

def make_optimizer(name, model, lr=1e-3):
    name = name.lower()
    if name == "adam":    return optim.Adam(model.parameters(), lr=lr)
    if name == "sgd":     return optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    if name == "rmsprop": return optim.RMSprop(model.parameters(), lr=lr, momentum=0.9)
    raise ValueError("optimizer must be one of Adam/SGD/RMSProp")

"""**Results**"""

BASE = {
    "arch":        "lstm",
    "activation":  "relu",
    "optimizer_name": "adam",
    "seq_len":     50,
    "grad_clip":   True,
    "epochs":      3,
    "batch_size":  32,
    "lr":          1e-3
}

SWEEPS = {
    "arch":       ["rnn", "lstm", "bilstm"],
    "activation": ["sigmoid", "relu", "tanh"],
    "optimizer_name":  ["adam", "sgd", "rmsprop"],
    "seq_len":    [25, 50, 100],
    "grad_clip":  [False, True],
}

os.makedirs("/content/results/plots", exist_ok=True)

def run_experiment(arch, activation, optimizer_name, seq_len, grad_clip, epochs, batch_size, lr):
    train_ld, test_ld = make_loaders(seq_len, batch_size=batch_size)
    model = SentimentModel(arch=arch, activation=activation).to(device)
    criterion = nn.BCELoss()
    optimizer = make_optimizer(optimizer_name, model, lr=lr)

    hist = {"train_loss": [], "val_loss": [], "val_acc": [], "val_f1": [], "epoch_times": []}

    for ep in range(1, epochs+1):
        clip_norm = 1.0 if grad_clip else None  # default clipping strength
        tr_loss, ep_time = train_one_epoch(model, train_ld, optimizer, criterion, clip_norm=clip_norm)
        val_loss, val_acc, val_f1 = evaluate(model, test_ld, criterion)
        hist["train_loss"].append(tr_loss)
        hist["val_loss"].append(val_loss)
        hist["val_acc"].append(val_acc)
        hist["val_f1"].append(val_f1)
        hist["epoch_times"].append(ep_time)

    return {
        "Model": arch.upper(),
        "Activation": activation.upper(),
        "Optimizer": optimizer_name.upper(),
        "Seq Length": seq_len,
        "Grad Clipping": "Yes" if grad_clip else "No",
        "Accuracy": round(hist["val_acc"][-1], 4),
        "F1": round(hist["val_f1"][-1], 4),
        "Epoch Time (s)": round(np.mean(hist["epoch_times"]), 2),
        "train_loss_curve": hist["train_loss"],
        "val_loss_curve": hist["val_loss"],
    }

all_results = []

# 1) Vary architecture
for v in SWEEPS["arch"]:
    cfg = BASE.copy(); cfg["arch"]=v
    res = run_experiment(**cfg); all_results.append(res); gc.collect()

# 2) Vary activation
for v in SWEEPS["activation"]:
    cfg = BASE.copy(); cfg["activation"]=v
    res = run_experiment(**cfg); all_results.append(res); gc.collect()

# 3) Vary optimizer
for v in SWEEPS["optimizer_name"]:
    cfg = BASE.copy(); cfg["optimizer_name"]=v
    res = run_experiment(**cfg); all_results.append(res); gc.collect()

# 4) Vary sequence length
for v in SWEEPS["seq_len"]:
    cfg = BASE.copy(); cfg["seq_len"]=v
    res = run_experiment(**cfg); all_results.append(res); gc.collect()

# 5) Vary grad clipping
for v in SWEEPS["grad_clip"]:
    cfg = BASE.copy(); cfg["grad_clip"]=v
    res = run_experiment(**cfg); all_results.append(res); gc.collect()

df_res = pd.DataFrame([{k:v for k,v in r.items() if not k.endswith("_curve")} for r in all_results])

print("=== Summary (last epoch per run) ===")
disp = df_res[["Model","Activation","Optimizer","Seq Length","Grad Clipping","Accuracy","F1","Epoch Time (s)"]]\
       .sort_values(["Model","Seq Length","Activation","Optimizer","Grad Clipping"]).reset_index(drop=True)
print(disp.to_string(index=False))

best = df_res.sort_values("F1", ascending=False).head(1)
worst = df_res.sort_values("F1", ascending=True).head(1)
print("\n=== Best (by F1) ===")
print(best.to_string(index=False))
print("\n=== Worst (by F1) ===")
print(worst.to_string(index=False))

"""**Plots**"""

import matplotlib.pyplot as plt

# Build a single boolean mask (no chained []), and use optimizer_name
mask = (
    (df_res["Model"] == BASE["arch"].upper()) &
    (df_res["Activation"] == BASE["activation"].upper()) &
    (df_res["Optimizer"] == BASE["optimizer_name"].upper()) &
    (df_res["Grad Clipping"] == ("Yes" if BASE["grad_clip"] else "No"))
)

sub = df_res.loc[mask].copy()

# If that exact combo isn't present (e.g., you changed SWEEPS),
# gracefully fall back to same arch only.
if sub.empty:
    sub = df_res.loc[df_res["Model"] == BASE["arch"].upper()].copy()

sub = sub.sort_values("Seq Length")

# A) Accuracy/F1 vs Sequence Length
plt.figure()
plt.plot(sub["Seq Length"], sub["Accuracy"], marker='o', label='Accuracy')
plt.plot(sub["Seq Length"], sub["F1"], marker='o', label='F1 (macro)')
plt.xlabel("Sequence Length"); plt.ylabel("Score"); plt.title("Accuracy / F1 vs Sequence Length")
plt.legend(); plt.grid(True)
os.makedirs("/content/results/plots", exist_ok=True)
plt.savefig("/content/results/plots/acc_f1_vs_seq_len.png", bbox_inches="tight")
plt.show()

# B) Training Loss vs Epochs for best and worst models (by F1)

# Helper to make a key from a df row
def row_key(row):
    return (row["Model"], row["Activation"], row["Optimizer"], int(row["Seq Length"]), row["Grad Clipping"])

# Map from (Model,Activation,Optimizer,SeqLen,GradClip) -> curves saved in all_results
curves = {}
for r in all_results:
    k = (r["Model"], r["Activation"], r["Optimizer"], int(r["Seq Length"]), r["Grad Clipping"])
    curves[k] = {"train": r["train_loss_curve"], "val": r["val_loss_curve"]}

# Get best/worst rows by F1
best_row  = df_res.sort_values("F1", ascending=False).iloc[0]
worst_row = df_res.sort_values("F1", ascending=True ).iloc[0]
best_key  = row_key(best_row)
worst_key = row_key(worst_row)

# Plot best model training loss
plt.figure()
plt.plot(curves[best_key]["train"], marker='o')
plt.xlabel("Epoch"); plt.ylabel("Training Loss")
plt.title(f"Best Model Train Loss\n{best_key}")
plt.grid(True)
plt.savefig("/content/results/plots/best_train_loss.png", bbox_inches="tight")
plt.show()

# Plot worst model training loss
plt.figure()
plt.plot(curves[worst_key]["train"], marker='o')
plt.xlabel("Epoch"); plt.ylabel("Training Loss")
plt.title(f"Worst Model Train Loss\n{worst_key}")
plt.grid(True)
plt.savefig("/content/results/plots/worst_train_loss.png", bbox_inches="tight")
plt.show()

#key metrics
disp = df_res[["Model","Activation","Optimizer","Seq Length","Grad Clipping","Accuracy","F1","Epoch Time (s)"]] \
       .sort_values(["Model","Seq Length","Activation","Optimizer"]).reset_index(drop=True)
print(disp.to_string(index=False))